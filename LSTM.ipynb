{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qaJPZFHOkyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pickle, keras\n",
        "import pandas as pd\n",
        "from keras.models import Sequential, model_from_json\n",
        "from keras.layers import LSTM, Dense,Embedding, Dropout, GRU, BatchNormalization, Conv2D, MaxPooling2D, Activation\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.optimizers import *\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtEpzMh2OvYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rloAUAl7OxgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets load data from pickle file we previously stored\n",
        "data_file = '/content/drive/My Drive/pickles/train_berkeley_skeleton.pickle' # redefined varaible in case you have completed above steps previously.\n",
        "print('Trying to load pickle from %s' % data_file)\n",
        "with open(data_file, 'rb') as file:\n",
        "    svhn_datasets = pickle.load(file)\n",
        "    train_dataset = svhn_datasets['train_dataset']\n",
        "    \n",
        "print('Pickle Loaded Successfully!')\n",
        "del svhn_datasets\n",
        "x_train = train_dataset['X_train']\n",
        "y_train = train_dataset['Y_train']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDlSafcfO_-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# num_img = []\n",
        "# for i in range(len(x_train)):\n",
        "#   [m,n] = x_train[i].shape\n",
        "#   num_img.append(m)\n",
        "#   print(m)\n",
        "x_all = pd.concat(x_train).values\n",
        "Y = pd.concat(y_train).values\n",
        "print(x_all.shape)\n",
        "print(Y.shape)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "x_all = scaler.fit_transform(x_all)\n",
        "X=x_all.reshape(x_all.shape[0], 1, x_all.shape[1])\n",
        "\n",
        "# Y = np.array(list(map(one_hot_vec, Y)))\n",
        "Y = keras.utils.to_categorical(Y, num_classes=11)\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G06mCPwPCt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model(lr):\n",
        "        \n",
        "    model = Sequential()\n",
        "    dropout = 0.5\n",
        "    activation = 'elu'\n",
        "    model.add(LSTM(units= 64,return_sequences= True, activation=activation, input_shape=(X.shape[1], X.shape[2])))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(LSTM(units= 64, activation='relu'))\n",
        "    model.add(Dropout(dropout))\n",
        " #   model.add(LSTM(16, activation= 'relu'))\n",
        "    model.add(Dense(64, activation=activation))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(32, activation=activation))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(11, activation='softmax'))\n",
        "    opt = keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model\n",
        "model = make_model(0.002)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu0iPyrCPmru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X, Y,epochs = 10 , batch_size=64)\n",
        "plt.plot(history.history['acc'])\n",
        "# plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qnyWs0SDDCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_file = '/content/drive/My Drive/pickles/test_berkeley_skeleton.pickle' # redefined varaible in case you have completed above steps previously.\n",
        "print('Trying to load pickle from %s' % data_file)\n",
        "with open(data_file, 'rb') as file:\n",
        "    svhn_datasets = pickle.load(file)\n",
        "    test_dataset = svhn_datasets['test_dataset']\n",
        "    \n",
        "print('Pickle Loaded Successfully!')\n",
        "del svhn_datasets\n",
        "x_test = test_dataset['X_test']\n",
        "y_test = test_dataset['Y_test']\n",
        "\n",
        "x_all = pd.concat(x_test).values\n",
        "Y = pd.concat(y_test).values\n",
        "print(x_all.shape)\n",
        "print(Y.shape)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "x_all = scaler.fit_transform(x_all)\n",
        "X=x_all.reshape(x_all.shape[0], 1, x_all.shape[1])\n",
        "\n",
        "# Y = np.array(list(map(one_hot_vec, Y)))\n",
        "Y = keras.utils.to_categorical(Y, num_classes=11)\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3YLP52qDy6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = model.evaluate(X, Y, batch_size=64, verbose=1)\n",
        "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzdXFfgBP9qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_json = model.to_json()\n",
        "with open('drive/My Drive/models/rnn_6.json', 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights('drive/My Drive/models/rnn_6.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}